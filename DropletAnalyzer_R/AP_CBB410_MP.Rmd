---
title: "AP_CBB410_MTP"
author: "Arjun S. Pamidi"
date: "`r Sys.Date()`"
output: pdf_document
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*.

# Introduction

This R Notebook analyzes data generated by me and my lab buddy, Meng-han Pan, in the Diercks Lab here at Scripps. Specifically, we've recently been trying to accomplish controlled picoinjection of reagents into
microfluidic droplets. The problem is that these injections are too fast to see and measure by eye -- injections occur between 60 - 200 times per second. While the instrument that does the injections
offers image-based analysis of droplets within a specified area of the microscope, and can export a size distribution of the captured droplets, there are two issues. 1.) We do not know which droplet
a size measurement came from; the data is lost. Thus, we don't know if the instrument calculated the size correctly for a given droplet (it uses neural-network based recognition, which sometimes gets
it wrong.) 2.) The instrument does not measure the darkness of the droplet. This information is important because it can tell us whether or not we actually injected the droplet (if the droplet starts off
dark, and we inject water, the droplet gets lighter). Uninjected droplets remain dark and small, spontaneously injections are small and clear, and succesfully injected droplets are larger and of medium darkness.

Fortunately, the instrument saves a 'collage' of droplet images, each of which ideally contains one droplet as an image. To analyze the droplets with high accuracy, I wrote a c++ program using OpenCV with help from LLM's (my favorite
LLM-enabled IDE is Cursor; this usually outperforms an LLM by itself as Cursor automatically optimizes how the LLM is prompted, what information the LLM has access to, etc.). This program took significant development time (~12 hours)
but it was worth it because it's necessary for my main project in lab.

Essentially, you point the program
towards a folder on your computer which contains sub-folders, each of which contains a collage for one experiment. We have 3 independent variables: The emulsion flowrate (f_emulsion), the injectant flowrate (f_inject),
and the electrocoalescence voltage (Voltage), and two dependent variables (droplet diamter and droplet grayval (the lightness)), making 5 variables in total. 

For each image within the folder, the OpenCV program blurs the image, upscales it, enhances the contrast, and applies a lightness threshold as preprocessing steps. These make
the next step, Hough circle detection, more accurate. After the circle detection step, the program attempts to hone-in on the inner boundary of the droplet by scanning inwards and outwards on a path perpendicular to 8 points
of the droplet's perimeter to find exactly the inner portion of the droplet's boundary by looking at the lightness of each pixel; there is typically a transition from dark to light when you cross the boundary, which is dark.

At that point, the program has honed in on the most accurate circle it could find representing the inner portion of the droplet. It uses that circle to measure the average grayval within that circle and that circle's diameter. It then
captures an image of the original droplet but with the circle it detected overlaid on top, for quality control purposes. It names the image according the droplet's index, radius, and grayval and puts it in an outputs folder within the
folder containing the collage of images for that experiment.

The final (for now :D) program can analyze ~30,000 droplets in 30 minutes assuming parallelization by running the program on multiple cores; my machine has 16 physical cores (Ryzen 9950X3D) so each core can analyze one collage -> 16 collages simultaneously. 

Next, to prepare the data for analysis in R, I wrote a simple python script. (This maybe could have been done in R, but I think it would have been a pain. For this sort of task, I felt python would be way easier to write and debug even
with help from Cursor).
You point this script at a csv sheet which links the name of each folder to the experimental conditions used to generate those droplets, and a path to the folder
containing the folders with droplet images in them. The python script then outputs the data in as 'tidy' a format I could think of (you'll see below) in the form of a .csv file. Now, the dataset is ready for analysis in R. The .csv is simply kept in working directory of the R Notebook in a folder called 'Data'.

# Part 1: Data import, cleaning, and mutation.

```{r}

library(tidyverse)
library(ggplot2)
library(dplyr)

# Read the CSV data from the data folder in the RMD directory.
droplet_data <- read_csv(
  "Data/PICOINJ_3_Results.csv",
  col_types = cols(),
  na = c("","NA")
)

# Let's display the first 10 rows of the tibble.
head(droplet_data)

# Let's tidy up the column headers and get rid of the '?', '()', and '.' 's.
droplet_data <- droplet_data %>%
  rename(
    code = 'Code',
    measured = 'Measured?',
    index = 'Index',
    valid = 'Valid?',
    radius_um = 'Radius (um)',
    voltage = 'Voltage',
    f_oil = 'Foil',
    f_emulsion = 'Femulsion',
    f_inject = 'Finject',
    grayval = 'GrayVal (A.U.)',
  )
# This should look better.
head(droplet_data)

# Just to make sure all the data managed to get in here, let's count the number of unique values for f_oil, f_emulsion, f_inject, voltage, and valid.
droplet_data %>%
  distinct(f_oil)

droplet_data %>%
  distinct(f_emulsion)
droplet_data %>%
  distinct(f_inject)
droplet_data %>%
  distinct(voltage)

droplet_data %>%
  distinct(valid)

# Yep, looking at the unique values for each variable shows me that indeed all of the experiments made it into the final csv.
# Here is a quick summary of the data:

droplet_data %>%
  summarise(
    total_experiments = n_distinct(code),
    total_droplets = n(),
    valid_droplets = sum(valid == "VALID", na.rm = TRUE),
    n_f_oil = n_distinct(f_oil),
    n_f_emulsion = n_distinct(f_emulsion),
    n_f_inject = n_distinct(f_inject),
    n_voltage = n_distinct(voltage)
  )

# A goal of this experiment was to find out what the injection volume is. Before we can figure that out, we need to know the droplet volumes.
# Thus, I'll mutate droplet_data to add a column with the droplet volume in picoliters.

droplet_data <- droplet_data %>%
  mutate(
    volume_pL = (4/3) * pi * radius_um^3 * (1e-3)
  )

# Now let's see if that worked.
droplet_data %>%
  select(radius_um, volume_pL)

head(droplet_data)

# Great, that worked. To start with the exploratory analysis, I want to plot histograms of droplet diameters for each electrocoalescence voltage, and
# color the points by grayval.

# Histogram stuff start -------------------------------------------------------

# I will use the Freedman-Diaconis rule to determine bin width. This will be most easily implemented with a function.
# Function accepts one input, x, which is a vector of the observations in the dataset.

fd_binwidth <- function(x) {
  # Use IQR, from stats, to get interquartile range. Remove NA's.
  iqr_val <- IQR(x, na.rm = TRUE)
  # Get size of vector x so we can make sure we have enough observations.
  n <- sum(!is.na(x))
  # Don't want to have invalid bin widths caused by too few droplets.
  if (n < 2 || iqr_val == 0) {
    # If too few droplets, just make the bin width 1/10th the range of the values.
    return(diff(range(x, na.rm = TRUE)) / 10)
  }
  # If the data had enough points, use the FD rule to find it and return it.
  bw <- 2 * iqr_val / (n^(1/3))
  return(bw)
}

# Now that we have this function, let's make a tibble with each experiment code string, the number of valid droplets,
# and the binwidth we should use when plotting the histogram.

# Making the new tibble:
droplet_binwidths <- droplet_data %>%
  # Filter out rows where the droplet was invalid.
  filter(valid == "VALID") %>%
  # Group by unique experiment codes.
  group_by(code) %>%
  # For each group, summarise will count the valid droplets by just counting
  # how many observations the group has (we already filtered out the invalid
  # droplets). When summarise operates on a grouped tibble, functions like
  # fd_binwidth() receive only the values from that specific group.
  # Thus, we can pass the radius_um column into the fd_binwidth function,
  # and it will calculate a binwidth for each group's radius values separately,
  # giving us a new column called binwidth.
  summarise(
    valid_droplets = n(),
    binwidth = fd_binwidth(radius_um)
  )

head(droplet_binwidths)
# Okay, looks like a good tibble. Now, let's prepare a version of droplet_data
# specifically for the histograms (only valid droplets).

valid_droplet_data <- droplet_data %>%
  filter(valid == "VALID")

# Since I want to make one set of histograms per voltage level, we should
# identify each voltage level from the dataset:

voltage_levels <- sort(unique(valid_droplet_data$voltage))

# Display this to check we've got all three:
voltage_levels

# Going to make the 250V histogram first.

v1 <- voltage_levels[1]
histogram_data_v1 <- valid_droplet_data %>%
  filter(voltage == v1)

# See if we only have the data from 250V:
table(histogram_data_v1$voltage)

# We have only one value for Voltage in this tibble and it has 8889
# observations associated with it. Let's now join the binwidth onto this tibble.

histogram_data_v1 <- histogram_data_v1 %>%
  left_join(droplet_binwidths, by = 'code')

# And let's see if it makes sense:
histogram_data_v1 %>%
  distinct(code,binwidth)

# Yep, matches the previous binwidth table. It's just that now we only have the
# 250V data.
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
